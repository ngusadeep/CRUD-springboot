{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMSWkUc9ox7JDnm2afKPDT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngusadeep/CRUD-springboot/blob/main/docs_parser_with_deepseek_ocr_3b_model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“„ OCR + Structured Document Parser Prototype**\n",
        "### Using DeepSeek-OCR:3B (Ollama)\n",
        "\n",
        "### This notebook demonstrates a complete prototype pipeline for an OCR-based document parser designed for processing Tanzanian shipment documents (EIRs, Export Orders, etc.).\n",
        "\n",
        "### **Core Functions**\n",
        "- Accept **any document format**: `.jpg, .jpeg, .png, .webp, .tiff, .bmp, .pdf`\n",
        "- If the uploaded file is a **PDF**, automatically convert it to images\n",
        "- Use **Ollama + DeepSeek-OCR:3B** locally inside Google Colab\n",
        "- Process image(s) and extract clean text\n",
        "- Output **structured with desired fomart**\n",
        "\n",
        "Letâ€™s begin!\n"
      ],
      "metadata": {
        "id": "zVFN2vW8rD0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Install Dependencies\n"
      ],
      "metadata": {
        "id": "GJOGiAK8y1U_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama (server + CLI)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install poppler-utils for PDF -> images conversion\n",
        "!apt-get update -qq && apt-get install -y -qq poppler-utils\n",
        "\n",
        "# Python packages\n",
        "!pip install -q pdf2image pillow pandas openpyxl"
      ],
      "metadata": {
        "id": "NSdqbjCZ8yPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Ollama (background) and pull the model"
      ],
      "metadata": {
        "id": "i_7Uz3HD05kF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ebeb212"
      },
      "source": [
        "!ollama serve &>/content/ollama.log & sleep 1\n",
        "!ollama pull deepseek-ocr:3b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Imports , libraries and constants"
      ],
      "metadata": {
        "id": "QsuU8sC98jsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "import base64, subprocess, json, os, tempfile, re, io, time\n",
        "import pandas as pd\n",
        "\n",
        "# Where outputs will be stored\n",
        "OUTPUT_DIR = \"/content/ocr_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "2NEgv47w8i7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System PROMPT"
      ],
      "metadata": {
        "id": "byaXQiE92GK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MASTER_SYSTEM_PROMPT = r\"\"\"\n",
        "You are a precise and reliable OCR + document parser AI that extracts\n",
        "structured information from ANY type of scanned document or image, including:\n",
        "- shipment documents\n",
        "- container EIRs\n",
        "- export orders\n",
        "- invoices\n",
        "- receipts\n",
        "- IDs\n",
        "- contracts\n",
        "- PDFs converted to images\n",
        "\n",
        "Your responsibilities:\n",
        "1. Read the user's instruction and extract ONLY the requested information.\n",
        "2. Understand the structure and layout of the uploaded document.\n",
        "3. Use intelligent field detection even if the document format changes.\n",
        "4. Be robust to low-quality scans, rotated images, blur, stamps, handwriting.\n",
        "5. Always respond in STRICT JSON unless the user specifies a different format.\n",
        "\n",
        "RULES:\n",
        "- Never add explanations or text outside the JSON or chosen output format.\n",
        "- If a requested field does not exist, return it as null.\n",
        "- If multiple pages exist, return an array of results by page_number.\n",
        "- Maintain consistent keys, casing, and values.\n",
        "- Do NOT hallucinate values.\n",
        "\n",
        "You will receive:\n",
        "1. The user's extraction instruction in natural language.\n",
        "2. The document image (base64).\n",
        "3. Optional preview metadata.\n",
        "\n",
        "Your task:\n",
        "- Parse the document.\n",
        "- Extract exactly what the user asked for.\n",
        "- Output in the requested format (JSON/CSV/TXT/MD/YAML).\n",
        "\n",
        "If the user instruction is vague:\n",
        "- Ask for clarification in a single short question.\n",
        "\n",
        "You are optimized for DeepSeek-OCR:3B running inside Ollama.\n",
        "\n",
        "\"\"\"\n",
        "print(\"Master prompt loaded (hidden).\")"
      ],
      "metadata": {
        "id": "ND3f10D42PsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Upload"
      ],
      "metadata": {
        "id": "LFR5OXAs3BlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Upload your document here\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded. Re-run cell and upload a file.\")\n",
        "\n",
        "uploaded_filename = list(uploaded.keys())[0]\n",
        "local_path = \"/content/\" + uploaded_filename\n",
        "with open(local_path, \"wb\") as f:\n",
        "    f.write(uploaded[uploaded_filename])\n",
        "\n",
        "print(\"Saved to:\", local_path)"
      ],
      "metadata": {
        "id": "ENoxhBvq3FX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert pdf to images"
      ],
      "metadata": {
        "id": "eJaz3LdD4Wrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_images(pdf_path, dpi=200, out_dir=\"/content/pdf_pages\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    pages = convert_from_path(pdf_path, dpi=dpi)\n",
        "    paths = []\n",
        "    for i, page in enumerate(pages):\n",
        "        p = os.path.join(out_dir, f\"page_{i+1}.png\")\n",
        "        page.save(p, \"PNG\")\n",
        "        paths.append(p)\n",
        "    return paths\n",
        "\n",
        "def prepare_images(file_path):\n",
        "    ext = file_path.lower().split('.')[-1]\n",
        "    if ext == \"pdf\":\n",
        "        print(\"Converting PDF to images...\")\n",
        "        return pdf_to_images(file_path)\n",
        "    else:\n",
        "        return [file_path]\n",
        "\n",
        "image_paths = prepare_images(local_path)\n",
        "print(\"Prepared images:\", image_paths)"
      ],
      "metadata": {
        "id": "ZpQJVHIh4Zsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in image_paths:\n",
        "    print(\"----\", p)\n",
        "    try:\n",
        "        img = Image.open(p)\n",
        "        display(img.resize((800, int(800 * img.height / img.width))))\n",
        "    except Exception as e:\n",
        "        print(\"Error displaying image:\", e)"
      ],
      "metadata": {
        "id": "kvPmISLE4o4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_data_uri(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        b = f.read()\n",
        "    b64 = base64.b64encode(b).decode(\"utf-8\")\n",
        "    # assume PNG/JPEG by extension\n",
        "    ext = path.split('.')[-1].lower()\n",
        "    mime = \"image/png\" if ext in (\"png\", \"svg\") else \"image/jpeg\"\n",
        "    return f\"data:{mime};base64,{b64}\""
      ],
      "metadata": {
        "id": "6rzLW7-E4zba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_instruction = \"\"\"\n",
        "Extract the following fields from the document image:\n",
        "- container_terminal\n",
        "- shipment_date\n",
        "- shipment_number\n",
        "- container_number\n",
        "- container_size\n",
        "\"\"\"\n",
        "\n",
        "print(\"Choose your desired output format (json, csv, xlsx, txt). Default is csv:\")\n",
        "user_fmt = input().strip().lower()\n",
        "if user_fmt in (\"json\", \"csv\", \"xlsx\", \"txt\"):\n",
        "    fmt = user_fmt\n",
        "else:\n",
        "    fmt = \"csv\"\n",
        "print(\"Extraction instruction set to extract all shipment info.\")\n",
        "print(\"Output format:\", fmt)\n"
      ],
      "metadata": {
        "id": "bYYmuVSn47NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_payload(system_prompt, user_instruction, base64_image):\n",
        "    \"\"\"\n",
        "    Prepare the prompt structure expected by the model.\n",
        "    We include system prompt + user content that contains instruction and image.\n",
        "    \"\"\"\n",
        "    # Keep message structure simple: system (context) + user (instruction + image)\n",
        "    user_content = [\n",
        "        {\"type\": \"text\", \"text\": user_instruction},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": base64_image}},\n",
        "    ]\n",
        "    return system_prompt, user_content"
      ],
      "metadata": {
        "id": "u2v_RDuU6m9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_deepseek_on_image(image_path, system_prompt, user_instruction, max_retries=2):\n",
        "    data_uri = image_to_data_uri(image_path)\n",
        "    system, user_content = build_model_payload(system_prompt, user_instruction, data_uri)\n",
        "\n",
        "    # prepare ollama CLI call - we'll stream the prompt via stdin\n",
        "    # Ollama expects text input; some setups accept JSON-like chat payloads; we use simple interaction\n",
        "    prompt_text = json.dumps({\n",
        "        \"system\": system,\n",
        "        \"user\": user_content\n",
        "    })\n",
        "    cmd = [\"ollama\", \"run\", \"deepseek-ocr:3b\"]\n",
        "\n",
        "    for attempt in range(1, max_retries+1):\n",
        "        try:\n",
        "            proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            out, err = proc.communicate(input=prompt_text.encode(), timeout=120)\n",
        "            output_text = out.decode(errors=\"ignore\").strip()\n",
        "            if not output_text:\n",
        "                output_text = err.decode(errors=\"ignore\").strip()\n",
        "            # try to find JSON in output_text\n",
        "            return output_text\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "            if attempt == max_retries:\n",
        "                raise RuntimeError(\"DeepSeek-OCR call timed out.\")\n",
        "            print(\"Retrying... attempt\", attempt+1)\n",
        "    raise RuntimeError(\"Failed to call DeepSeek-OCR\")\n"
      ],
      "metadata": {
        "id": "b2u7BG4T6qnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_model_outputs = []\n",
        "start = time.time()\n",
        "\n",
        "for i, img_path in enumerate(image_paths):\n",
        "    print(f\"Processing page {i+1}/{len(image_paths)} ...\")\n",
        "    try:\n",
        "        raw = run_deepseek_on_image(img_path, MASTER_SYSTEM_PROMPT, user_instruction)\n",
        "        print(\"Model raw output (first 400 chars):\")\n",
        "        print(raw[:400])\n",
        "        raw_model_outputs.append({\"page_number\": i+1, \"image_path\": img_path, \"raw_output\": raw})\n",
        "    except Exception as e:\n",
        "        print(\"Error processing page:\", e)\n",
        "        raw_model_outputs.append({\"page_number\": i+1, \"image_path\": img_path, \"raw_output\": None, \"error\": str(e)})\n",
        "\n",
        "duration = time.time() - start\n",
        "print(f\"Done. Time elapsed: {duration:.1f}s\")\n"
      ],
      "metadata": {
        "id": "aNnpVd6T6unR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}