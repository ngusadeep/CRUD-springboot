{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngusadeep/CRUD-springboot/blob/main/qwen2_5vl_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install ollama pdf2image pydantic\n",
        "%pip install colab-xterm\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y pciutils lshw poppler-utils"
      ],
      "metadata": {
        "id": "EYRDY4D2CdMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "QB4LlWtx_fLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "import threading\n",
        "from pprint import pprint\n",
        "def run_ollama():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "thread = threading.Thread(target=run_ollama)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "wZ7gSDTWMM9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull qwen2.5vl:7b\n",
        "!ollama list\n",
        "!curl http://localhost:11434/v1/models/qwen2.5vl:7b"
      ],
      "metadata": {
        "id": "wKXnFZIzNt2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import base64\n",
        "import random\n",
        "from pathlib import Path\n",
        "from pdf2image import convert_from_path\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "OLLAMA_URL = \"http://127.0.0.1:11434/api/generate\""
      ],
      "metadata": {
        "id": "8cpLTzGHySUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file():\n",
        "    uploads = files.upload()\n",
        "    original_filename = list(uploads.keys())[0]\n",
        "\n",
        "    # normalize filename\n",
        "    cleaned_filename = original_filename.lower().replace(\" \", \"_\")\n",
        "    cleaned_filename = re.sub(r\"[^a-z0-9_.-]\", \"\", cleaned_filename)\n",
        "\n",
        "    # handle duplicate filenames safely\n",
        "    base, ext = os.path.splitext(cleaned_filename)\n",
        "    counter = 1\n",
        "    final_filename = cleaned_filename\n",
        "\n",
        "    while os.path.exists(final_filename):\n",
        "        final_filename = f\"{base}_{counter}{ext}\"\n",
        "        counter += 1\n",
        "\n",
        "    # rename file on disk if needed\n",
        "    if final_filename != original_filename:\n",
        "        os.rename(original_filename, final_filename)\n",
        "\n",
        "    print(f\"‚úÖ Uploaded: {final_filename}\")\n",
        "    return final_filename"
      ],
      "metadata": {
        "id": "QAtuinqJPrbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_image_file(filename):\n",
        "    return filename.lower().endswith(('.png' , '.jpeg' , '.jpg'))"
      ],
      "metadata": {
        "id": "71C_J0WZQEWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pdf_pages_as_images(pdf_path):\n",
        "    # base filename (without extension)\n",
        "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "\n",
        "    # create a folder\n",
        "    output_dir = base_name\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # convert PDF pages to images\n",
        "    pages = convert_from_path(pdf_path)\n",
        "    image_paths = []\n",
        "\n",
        "    for i, page in enumerate(pages):\n",
        "        img_path = os.path.join(output_dir, f\"page{i+1}.png\")\n",
        "        page.save(img_path, 'PNG')\n",
        "        image_paths.append(img_path)\n",
        "\n",
        "    print(f\"üìÑ Extracted {len(image_paths)} pages from '{pdf_path}'\")\n",
        "    return image_paths"
      ],
      "metadata": {
        "id": "Fhs494piQbck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "        \"You are a precise and reliable OCR + document parser AI that extracts \"\n",
        "        \"structured shipment details from scanned container shipments documents, primarily \"\n",
        "        \"equipment interchange reports (EIRs) and export container orders for empty containers \"\n",
        "        \"being gated out from terminals in Tanzania.\\n\\n\"\n",
        "        \"Your job is to look at the uploaded image and extract ONLY these fields:\\n\"\n",
        "        \"- container_terminal\\n\"\n",
        "        \"- shipment_date (in YYYY-MM-DD format)\\n\"\n",
        "        \"- shipment_number\\n\"\n",
        "        \"- container_number\\n\"\n",
        "        \"- container_size (Should be either 20 or 40 only)\\n\\n\"\n",
        "        \"Mappings:\\n\"\n",
        "        \"Use this mapping below to know where to get each item above on each terminal document.\\n\\n\"\n",
        "        \"APM TERMINALS\\n\"\n",
        "        \"- container_terminal >> APM TERMINALS\\n\"\n",
        "        \"- shipment_number >> EIR No\\n\"\n",
        "        \"- shipment_date >> Gate Out Date\\n\"\n",
        "        \"- container_number >> Container No\\n\"\n",
        "        \"- container_size >> Size/Type\\n\"\n",
        "        \"FANTUZZI INVESTMENTS LTD\\n\"\n",
        "        \"- container_teminal >> FANTUZZI INVESTMENT LTD\\n\"\n",
        "        \"- shipment_number >> Request No.\\n\"\n",
        "        \"- shipment_date >> Date out\\n\"\n",
        "        \"- container_number >> Container No.\\n\"\n",
        "        \"- container_size >> Type\\n\"\n",
        "        \"KURASINI CONTAINER TERMINAL LTD\\n\"\n",
        "        \"- container_terminal >> KURASINI CONTAINER TERMINAL LTD\\n\"\n",
        "        \"shipment_number >> KCT/OUT/\\n\"\n",
        "        \"shipment_date >>  DATED\\n\"\n",
        "        \"container_number >> CONTAINER NUMBER\\n\"\n",
        "        \"container_size >> LENGTH\\n\"\n",
        "        \"ORION TRANSPORT (T) LTD\\n\"\n",
        "        \"container_terminal >> ORION TRANSPORT (T) LTD\\n\"\n",
        "        \"shipment_number >> Outward No\\n\"\n",
        "        \"shipment_date >> Date out\\n\"\n",
        "        \"container_number >> Container #\\n\"\n",
        "        \"container_size >> Container size\\n\"\n",
        "        \"TAZAMA PIPELINES LTD\\n\"\n",
        "        \"container_terminal  >> TAZAMA PIPELINES LTD\\n\"\n",
        "        \"shipment_number >> Bill of Lading\\n\"\n",
        "        \"shipment_date >> Interchange Date\\n\"\n",
        "        \"container_number  >> Container No\\n\"\n",
        "        \"container_size >> Container Size\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- Container size MUST be extracted as an integer, either 20 or 40. If the document shows a value like '22G1', extract the numerical part '20'. If '42G1', extract '40'.\\n\"\n",
        "        \"- Respond ONLY with a valid JSON object. No explanations, no text before or after.\\n\"\n",
        "        \"- If a field is missing or unclear, output it as null.\\n\"\n",
        "        \"- Be tolerant to printed partially obscured data.\\n\"\n",
        "    )\n",
        "\n",
        "user_prompt = (\n",
        "        \"Extract the container terminal, shipment date, shipment number, container number and container size from this document image. \"\n",
        "        \"Return strictly in JSON format.\"\n",
        "    )\n",
        "\n",
        "class ShipmentData(BaseModel):\n",
        "    container_terminal: str\n",
        "    shipment_date: str\n",
        "    shipment_number: str\n",
        "    container_number: str\n",
        "    container_size: int"
      ],
      "metadata": {
        "id": "awDlJxfFQlfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image_path, model=\"qwen2.5vl:7b\"):\n",
        "    # Read image\n",
        "    with open(image_path, \"rb\") as img:\n",
        "        img_b64 = base64.b64encode(img.read()).decode()\n",
        "\n",
        "    # Build Ollama payload\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": system_prompt + \"\\n\\n\" + user_prompt,\n",
        "        \"images\": [img_b64],\n",
        "        \"stream\": False\n",
        "    }\n",
        "\n",
        "    # Send to Ollama\n",
        "    res = requests.post(OLLAMA_URL, json=payload)\n",
        "\n",
        "    # Check for successful response and 'response' key\n",
        "    try:\n",
        "        response_json = res.json()\n",
        "        if \"response\" not in response_json:\n",
        "            print(\"‚ùå Ollama API response did not contain 'response' key.\")\n",
        "            print(\"Full Ollama API response:\")\n",
        "            pprint(response_json)\n",
        "            raise KeyError(\"Missing 'response' key in Ollama API output\")\n",
        "        data = response_json[\"response\"]\n",
        "\n",
        "        # Strip markdown code block wrappers if present\n",
        "        if data.strip().startswith('```json') and data.strip().endswith('```'):\n",
        "            data = data.strip()[len('```json'):-len('```')].strip()\n",
        "\n",
        "    except requests.exceptions.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Failed to decode JSON from Ollama API: {e}\")\n",
        "        print(\"Raw Ollama API response:\")\n",
        "        print(res.text)\n",
        "        raise e\n",
        "    except KeyError as e:\n",
        "        print(f\"‚ùå KeyError during Ollama response parsing: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # Parse into Pydantic model\n",
        "    try:\n",
        "        parsed = ShipmentData.model_validate_json(data)\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå JSON parse error:\", e)\n",
        "        print(\"Raw model output:\")\n",
        "        print(data)\n",
        "        raise e\n",
        "\n",
        "    # No cost in Ollama ‚Üí return None or custom info\n",
        "\n",
        "    return parsed"
      ],
      "metadata": {
        "id": "wTbRcQ_9QrWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Handles upload of either an image or a PDF.\n",
        "    - For image: extracts data from it.\n",
        "    - For PDF: splits into pages, processes each page.\n",
        "    - Writes results to a CSV file\n",
        "    \"\"\"\n",
        "\n",
        "    filename = upload_file()\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(\"\\n‚è≥ Processing...\")\n",
        "\n",
        "    # Define output CSV filename\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "    csv_filename = f\"{base_name}_extracted.csv\"\n",
        "\n",
        "    # Detect file type and extract page images if needed\n",
        "    if is_image_file(filename):\n",
        "        image_paths = [filename]\n",
        "    elif filename.lower().endswith(\".pdf\"):\n",
        "        image_paths = extract_pdf_pages_as_images(filename)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Only PDF, JPG, or PNG are allowed.\")\n",
        "\n",
        "    # Define CSV columns\n",
        "    csv_columns = [\"file\", \"page_number\", \"container_terminal\", \"shipment_date\", \"shipment_number\",  \"container_number\", \"container_size\"]\n",
        "    all_data = []\n",
        "\n",
        "    # Process each image\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        print(f\"‚û°Ô∏è Processing page {i+1}/{len(image_paths)}: {img_path}\")\n",
        "        data = process_image(img_path)\n",
        "        all_data.append({\n",
        "                \"file\": os.path.basename(filename),\n",
        "                \"page_number\": i + 1,\n",
        "                \"container_terminal\": data.container_terminal,\n",
        "                \"shipment_date\": data.shipment_date,\n",
        "                \"shipment_number\": data.shipment_number,\n",
        "                \"container_number\": data.container_number,\n",
        "                \"container_size\": data.container_size,\n",
        "            })\n",
        "\n",
        "\n",
        "    # Calculate total time\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    print(f\"‚úÖ Processing done.\\n\")\n",
        "    print(f\"‚è±Ô∏è Total time: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Save results to CSV\n",
        "    if all_data:\n",
        "        with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_data)\n",
        "\n",
        "        print(f\"\\n‚úÖ Data saved to: {csv_filename}\")\n",
        "        files.download(csv_filename)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No valid data extracted.\")"
      ],
      "metadata": {
        "id": "13o2a5JLSyvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "i_ERmRuFS-81"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ml-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}